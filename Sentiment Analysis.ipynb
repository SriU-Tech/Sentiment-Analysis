{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964eb7f7-4232-4596-addf-9c241dca81ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'AI is transforming the world around us into one where it was in my back pocket. I could no longer afford to buy cheap, new cars, I was worried about my house coming down. I lost my job, all the jobs I had had,'}] ['generated_text']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch  # <- This line ensures torch is in memory\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "output = generator(\"AI is transforming the world\", max_length=50)\n",
    "print(output,['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3261e06-cb4b-4134-b486-798f93b4d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xetNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading hf_xet-1.0.5-cp37-abi3-win_amd64.whl.metadata (498 bytes)\n",
      "Downloading hf_xet-1.0.5-cp37-abi3-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 2.6/4.2 MB 16.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.4/4.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.9/4.2 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.2/4.2 MB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.0.5\n"
     ]
    }
   ],
   "source": [
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c6ba58-d136-490c-892a-521deaae9d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9996370077133179}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Sentiment analysis using DistilBERT\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "result = classifier(\"I love working with AI!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0965b56-bb25-49cc-b2b1-0712cfc53697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fe1a50d-2c98-4778-8d11-edd773aeb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "input_data = Input(shape=(784,))\n",
    "hidden = Dense(256, activation='relu')(input_data)\n",
    "z_mean = Dense(2)(hidden)\n",
    "z_log_var = Dense(2)(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbee25cd-a830-4b81-8a28-74a4bed08b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], 2))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_hidden = Dense(256, activation='relu')\n",
    "decoder_output = Dense(784, activation='sigmoid')\n",
    "hidden_decoded = decoder_hidden(z)\n",
    "outputs = decoder_output(hidden_decoded)\n",
    "\n",
    "# Full VAE Model\n",
    "vae = Model(input_data, outputs)\n",
    "vae.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c75869d-6938-4bb7-b866-db87c02182d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sripa\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=(100,)),\n",
    "        layers.Dense(784, activation=\"sigmoid\")  # output: 28x28 flattened image\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=(784,)),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Instantiate\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
